{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74107aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "import random\n",
    "import sqlite3\n",
    "from estnltk import Text\n",
    "from estnltk_neural.taggers import StanzaSyntaxTagger\n",
    "from estnltk.taggers import NerTagger\n",
    "import json\n",
    "from estnltk.converters import text_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d878e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Layer\n",
    "from estnltk.taggers import Tagger\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3deb79",
   "metadata": {},
   "source": [
    "PhrasePatternTagger without NER patterns in ruleset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11576b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner(ner_layer, word_layer, span):\n",
    "        nertag = None\n",
    "        if len(ner_layer) > 0:\n",
    "            word = word_layer.get(span)\n",
    "            for n in ner_layer:\n",
    "                for part in n:\n",
    "                    if part==word:\n",
    "                        nertag=n.nertag\n",
    "        if nertag:\n",
    "            return nertag\n",
    "        return 'OTHER'\n",
    "    \n",
    "def get_POS(word_layer, span):\n",
    "    infinite_verb_forms = ['da', 'des', 'ma', 'maks', 'mas', 'mast', 'mata', 'nud', 'tav', 'tud', 'v']\n",
    "    # if POS is ambiguous, only unique tags are kept, e.g. ['V', 'A', 'A'] -> ['V', 'A']\n",
    "    pos_list = []\n",
    "    word = word_layer.get(span)\n",
    "    for i in range(len(word.morph_analysis['partofspeech'])):\n",
    "        if word.morph_analysis['partofspeech'][i] == 'V':\n",
    "            if word.morph_analysis['form'][i] in infinite_verb_forms:\n",
    "                pos_list.append('V_inf')\n",
    "            elif word.form[i] == 'neg':\n",
    "                pos_list.append('V_neg')\n",
    "            else:\n",
    "                pos_list.append('V_fin')\n",
    "        else:\n",
    "            pos_list.append(word.morph_analysis['partofspeech'][i])\n",
    "    \n",
    "    if len(pos_list) > 1:\n",
    "        char_unique = [char for indx, char in enumerate(pos_list) if char not in pos_list[:indx]]\n",
    "        if len(char_unique) < 2:\n",
    "            return char_unique[0]\n",
    "        return '|'.join(char_unique)\n",
    "    return pos_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a230b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhrasePatternTagger2(Tagger):\n",
    "    \"\"\"Tags phrases that match given syntax and part-of-speech pattern rules, and their corresponding patterns.\"\"\" \n",
    "    \n",
    "    conf_param = ['rules_file', 'ruleset_map']\n",
    "    \n",
    "    def __init__(self, rules_file: str,\n",
    "                       output_layer='phrase_patterns',\n",
    "                       morph_analysis_layer='morph_analysis',\n",
    "                       words_layer='words',\n",
    "                       syntax_layer='stanza_syntax',\n",
    "                       ner_layer='ner'):\n",
    "        \n",
    "        self.input_layers = [morph_analysis_layer, words_layer, syntax_layer, ner_layer]\n",
    "        self.output_layer = output_layer\n",
    "        self.output_attributes = ['extraction_pattern', 'ner_pattern', 'pattern_id', 'score', 'phrase_pattern_id', 'phrase_class']\n",
    "        self.rules_file = rules_file\n",
    "\n",
    "        ruleset_map = defaultdict(list)\n",
    "        \n",
    "        with open(rules_file, encoding='UTF-8') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            for row in reader:\n",
    "                info = [row['ID'], row['POS_pattern']]\n",
    "                ruleset_map[row['tree']].append(info)\n",
    "                \n",
    "        self.ruleset_map = ruleset_map\n",
    "\n",
    "    def _make_layer_template(self):\n",
    "        layer = Layer(name=self.output_layer,\n",
    "                      text_object=None,\n",
    "                      attributes=self.output_attributes,\n",
    "                      enveloping=self.input_layers[1],\n",
    "                      ambiguous=True)\n",
    "        return layer\n",
    "        \n",
    "    def _make_layer(self, text, layers, status):\n",
    "        layer = self._make_layer_template()\n",
    "        layer.text_object = text\n",
    "        \n",
    "        for i in range(len(layers[self.input_layers[2]])): # Iterate over 'stanza_syntax' layer\n",
    "            pattern_spans = []    \n",
    "            ids = []\n",
    "\n",
    "            pattern_spans.append(layers[self.input_layers[2]][i])\n",
    "            ids.append([layers[self.input_layers[2]][i]['id'], layers[self.input_layers[2]][i]['head']])\n",
    "                \n",
    "            for j in range(i + 1, len(layers[self.input_layers[2]])):\n",
    "                tree = []\n",
    "                pos = []\n",
    "                ner = []\n",
    "                for k in range(len(pattern_spans)):\n",
    "                    if layers[self.input_layers[2]][j] not in pattern_spans:\n",
    "                        if layers[self.input_layers[2]][j] in pattern_spans[k]['children'] or pattern_spans[k] in layers[self.input_layers[2]][j]['children'] or layers[self.input_layers[2]][j]['parent_span'] != None and layers[self.input_layers[2]][j]['parent_span'] == pattern_spans[k]['parent_span']:\n",
    "                            pattern_spans.append(layers[self.input_layers[2]][j])\n",
    "                            ids.append([layers[self.input_layers[2]][j]['id'], layers[self.input_layers[2]][j]['head']])\n",
    "                \n",
    "                # fixing word and head ID values\n",
    "                ids_for_pattern = copy.deepcopy(ids)\n",
    "                for k in range(len(ids_for_pattern)):\n",
    "                    temp = ids_for_pattern[k][0]\n",
    "                    ids_for_pattern[k][0] = k+1\n",
    "                    for l in range(len(ids)):\n",
    "                        if ids[l][1] == temp:\n",
    "                            ids_for_pattern[l][1] = ids_for_pattern[k][0]\n",
    "            \n",
    "                word_ids = [word_id[0] for word_id in ids_for_pattern]\n",
    "                for k in range(len(ids_for_pattern)):\n",
    "                    if ids_for_pattern[k][0] == ids_for_pattern[k][1]:\n",
    "                        ids_for_pattern[k][1] = 0\n",
    "                    elif ids_for_pattern[k][1] not in word_ids:\n",
    "                        ids_for_pattern[k][1] = 0\n",
    "                \n",
    "                # finding the root of current pattern and setting its deprel value as such\n",
    "                for k in range(len(pattern_spans)):\n",
    "                    deprel = pattern_spans[k].deprel\n",
    "                    if ids_for_pattern[k][1] == 0 and deprel != 'root':\n",
    "                        deprel = 'root'\n",
    "                    tree.append([str(ids_for_pattern[k][0]), str(ids_for_pattern[k][1]), deprel])\n",
    "                    # POS-tag is taken from morph_analysis layer\n",
    "                    pos.append(get_POS(layers[self.input_layers[1]], pattern_spans[k]))\n",
    "                    # nertag is taken from ner layer\n",
    "                    ner.append(get_ner(layers[self.input_layers[-1]], layers[self.input_layers[1]], pattern_spans[k]))                     \n",
    "                    \n",
    "                pattern = [\" \".join(word_info) for word_info in tree]\n",
    "                #print(pattern)\n",
    "                # check if tree pattern exists in ruleset map\n",
    "                if \",\".join(pattern) in self.ruleset_map.keys():\n",
    "                    #print(pattern, 'yes')\n",
    "                    pos_pattern = \"-\".join(pos)\n",
    "                    ner_pattern = \"-\".join(ner)\n",
    "                    # check if POS-sequence and NER-sequence exist in ruleset map with given tree pattern\n",
    "                    for el in self.ruleset_map[\",\".join(pattern)]:\n",
    "                        #print(el[1], pos_pattern, el[2], ner_pattern)\n",
    "                        if el[1] == pos_pattern:\n",
    "                            #print(pattern, 'yesyes')\n",
    "                            # add annotation\n",
    "                            layer.add_annotation([span.base_span for span in pattern_spans], \n",
    "                                                 extraction_pattern=\",\".join([\",\".join(pattern), pos_pattern]),\n",
    "                                                 ner_pattern=ner_pattern,\n",
    "                                                 pattern_id=el[0],\n",
    "                                                 score=None,\n",
    "                                                 phrase_pattern_id=None,\n",
    "                                                 phrase_class=None)         \n",
    "                \n",
    "        return layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469ac20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Tagger</h4>\n",
       "Tags phrases that match given syntax and part-of-speech pattern rules, and their corresponding patterns.\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>output layer</th>\n",
       "      <th>output attributes</th>\n",
       "      <th>input layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>PhrasePatternTagger2</td>\n",
       "      <td>phrase_patterns</td>\n",
       "      <td>('extraction_pattern', 'ner_pattern', 'pattern_id', 'score', 'phrase_pattern_id', 'phrase_class')</td>\n",
       "      <td>('morph_analysis', 'words', 'stanza_syntax', 'ner')</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<h4>Configuration</h4>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rules_file</th>\n",
       "      <td>indicator_patterns_ner_tree_pos_updated.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruleset_map</th>\n",
       "      <td>defaultdict(&lt;class 'list'&gt;, {'string': [['int64', 'string']], '1 2 nmod,2 0 root ..., type: &lt;class 'collections.defaultdict'&gt;, length: 7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "PhrasePatternTagger2(input_layers=('morph_analysis', 'words', 'stanza_syntax', 'ner'), output_layer=phrase_patterns, output_attributes=('extraction_pattern', 'ner_pattern', 'pattern_id', 'score', 'phrase_pattern_id', 'phrase_class'), rules_file=indicator_patterns_ner_tree_pos_updated.csv, ruleset_map=defaultdict(<class 'list'>, {'string': [['int64', 'string']], '1 2 nmod,2 0 root ..., type: <class 'collections.defaultdict'>, length: 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_tagger = PhrasePatternTagger2(rules_file='indicator_patterns_ner_tree_pos_updated.csv')\n",
    "pattern_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2480f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading resources index: 20.1kB [00:00, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "stanza_tagger = StanzaSyntaxTagger(input_type='morph_analysis', input_morph_layer='morph_analysis',\n",
    "                                   add_parent_and_children=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40700cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = NerTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f076fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('correct_noun_phrases.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea197df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT * FROM correct_phrase_patterns\")\n",
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea1ed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21492 fraasi märgendamiseks kulus PhrasePatternTaggeril 807.0359604358673 sekundit\n",
      "Tagger leidis 21770 fraasi\n"
     ]
    }
   ],
   "source": [
    "tagged_phrases = defaultdict(list)\n",
    "\n",
    "random.shuffle(rows)\n",
    "\n",
    "start = time.time()\n",
    "tagged_total = 0\n",
    "\n",
    "for row in rows:\n",
    "    text = Text(row[8]).tag_layer('morph_analysis')\n",
    "    ner_tagger.tag(text)\n",
    "    stanza_tagger.tag(text)\n",
    "    pattern_tagger.tag(text)\n",
    "    if len(text.phrase_patterns) > 0:\n",
    "        tagged_total+=len(text.phrase_patterns)\n",
    "        for pattern in text.phrase_patterns:\n",
    "            # at the moment, up to 100 samples for each pattern are kept and later saved in database\n",
    "            if len(tagged_phrases[pattern['extraction_pattern'][0]]) < 100:\n",
    "                tagged_phrases[pattern['extraction_pattern'][0]].append(text)\n",
    "                \n",
    "print(f\"{len(rows)} fraasi märgendamiseks kulus PhrasePatternTaggeril {time.time()-start} sekundit\")\n",
    "print(f\"Tagger leidis {tagged_total} fraasi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f64e7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08948ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1f3f24f54c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect(\"tagged_noun_phrases2.db\")\n",
    "cur = con.cursor()\n",
    "cur.execute('pragma encoding=UTF8')\n",
    "cur.execute(\"CREATE TABLE tagged_phrases(ID INTEGER PRIMARY KEY, extraction_pattern TEXT, ner_pattern TEXT, pattern_id INTEGER, raw_text TEXT, parent_phrase TEXT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc829913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in tagged_phrases:\n",
    "    for i in range(len(tagged_phrases[el])):\n",
    "        phrase_json = text_to_json(tagged_phrases[el][i])\n",
    "        for phrase in tagged_phrases[el][i].phrase_patterns:\n",
    "            p_lemmas = []\n",
    "            for span in phrase:\n",
    "                morph_word = tagged_phrases[el][i].morph_analysis.get(span)\n",
    "                # first lemma is always chosen\n",
    "                p_lemmas.append(morph_word.lemma[0])\n",
    "            raw_text = ' '.join([l for l in p_lemmas])\n",
    "            cur.execute(\"\"\"INSERT INTO tagged_phrases\n",
    "                                    (extraction_pattern, ner_pattern, pattern_id, raw_text, parent_phrase)\n",
    "                                    VALUES (?, ?, ?, ?, ?);\"\"\", (phrase['extraction_pattern'][0], phrase['ner_pattern'][0], phrase['pattern_id'][0], raw_text, phrase_json))\n",
    "    \n",
    "            con.commit()\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987756f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
