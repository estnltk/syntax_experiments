{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdff5f7c-46df-425c-a40c-7b679188f451",
   "metadata": {},
   "source": [
    "# v10 Kollokatsioonide kogumine koondkorpusest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6365c90-832e-4231-8566-59e44be2f0ca",
   "metadata": {},
   "source": [
    "Skripti töö tulemusena salvestatakse kogutud andmed sqlite formaadis andmebaasi tabelitesse.\n",
    "\n",
    "Failide sisu vaatamiseks saab kasutada tarkvara [sqlbrowser](https://sqlitebrowser.org/dl/) ja/või Python [sqlite3](https://docs.python.org/3/library/sqlite3.html) teeki.\n",
    "\n",
    "Kollokatsiooniandmete salvestamisel tabelisse jäeti välja kollokatsioonid, milles:\n",
    "\n",
    "* verb oli umbisikuline (<code>feats</code> sisaldas paramteetrit <code>imps</code>);\n",
    "* verbi aeg polnud ükski järgnevatest: <code>past</code>, <code>impf</code>, <code>pres</code> (<code>feats</code> ei sisaldanud parameetreid <code>past</code> ega <code>impf</code> ega  <code>pres</code>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af571e44-edc1-414a-8616-823256f4ea38",
   "metadata": {},
   "source": [
    "### Tabelid\n",
    "\n",
    "#### verb\\_compound\\_obl\\_koondkorpus\\_sentences\n",
    "\n",
    "| väli | tüüp  |  kirjeldus | näide | märkus |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| **id** | int | kollokatsiooni<br/>unikaalne ID| *56* | |\n",
    "| **verb** | text | tegusõna lemma | *kirjutama* | |\n",
    "| **verb_compound** | text | määrsõna(de) lemma(d) | *alla,kokku* | mitme määrsõna korral on lemmade eraldajaks koma ja lemmad on alfabeetilises järjestuses |\n",
    "| **obl_root** | text| täiendi juure lemma| *reede* | |\n",
    "| **obl_case** | text | täiendi juure kääne | *ad* | vt [Käänded](#käänded) |\n",
    "| **ner_loc** | text | täiendi fraasi asetus NER 'LOC' spani suhtes  | *intersects* | vt [OBL asetus](#obl_asetus) |\n",
    "| **ner_per** | text | --\\|\\|-- 'PER' spani suhtes | *match* | vt [OBL asetus](#obl_asetus) |\n",
    "| **ner_org** | text | --\\|\\|-- 'ORG' spani suhtes | *contains* | vt [OBL asetus](#obl_asetus) |\n",
    "| **timex** | text |  --\\|\\|-- TIMEX spani suhtes | *-* | vt [OBL asetus](#obl_asetus)|\n",
    "| **count** | int | kollokatsiooni esinemiste arv üle korpuse | *56* |  | \n",
    "| **phrase_type** | text | täiendi fraasi tüüp | *null* |   täitmata |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcce848-5c28-4366-afc0-46f468089188",
   "metadata": {},
   "source": [
    "#### verb\\_compound\\_obl\\_koondkorpus\\_sentences\\_examples\n",
    "\n",
    "| väli | tüüp  |  kirjeldus |\tnäide | märkus |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| colId\t| int\t| *56* |kollokatsiooni id tabelist **verb\\_compound\\_obl\\_koondkorpus\\_sentences**|\n",
    "| examples | text\t| 123,678,334| komadega eraldatud kollektsioonide ID-d, kus vastav kollokatsioon esines |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906b1c2-b396-45cb-9474-2f37ed800ebc",
   "metadata": {},
   "source": [
    "### OBL asetus\n",
    "| väärtus | kirjeldus  |  \n",
    "| --- | --- |\n",
    "| **-** |           OBL ei ole kautud ühegi spaniga|\n",
    "| **match** |      OBL span langeb kokku NER/TIMEX spaniga|\n",
    "| **contains** |    OBL spani sees on NER/TIMEX span|\n",
    "| **inside** |        OBL span on  NER/TIMEX spani sees|\n",
    "| **intersects** |  OBL span lõikub NER/TIMEX spaniga|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58804b50-ae00-4e56-bdd1-137a44e2da89",
   "metadata": {},
   "source": [
    "### Käänded \n",
    "\n",
    "| kääne|  nimetus| \n",
    "| --- | --- |\n",
    "| nom |  nimetav | \n",
    "| gen | omastav | \n",
    "| part | osastav | \n",
    "| adit | lühike sisseütlev | \n",
    "| ill | sisseütlev | \n",
    "| in |  seesütlev | \n",
    "| el |  seestütlev | \n",
    "| all |  alaleütlev |\n",
    "| ad |  alalütlev | \n",
    "| abl |  alaltütlev | \n",
    "| tr | saav | \n",
    "| term |  rajav | \n",
    "| es |  olev | \n",
    "| abes |  ilma | \n",
    "| kom | kaasa | \n",
    "| \\<käändumatu\\> | täiendil puudus kääne | \n",
    "\n",
    "Käänete notatsioon on pärit [EstCG](https://github.com/EstSyntax/EstCG) projektist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08632474-8e1b-427d-8ff9-e248e039577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda activate py38_estnltk1.7\n",
    "#hpc serveris peaks sobima estnltk_collocations_py38\n",
    "#!conda install --channel conda-forge pygraphviz\n",
    "#!conda install -c conda-forge ipywidgets\n",
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70b28ee-5e99-438b-a5cd-d4099004dc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230415-124549 Start.\n",
      "INFO:storage.py:41: connecting to host: 'postgres.keeleressursid.ee', port: '5432', dbname: 'estonian-text-corpora', user: 'zummy'\n",
      "INFO:storage.py:58: schema: 'estonian_text_corpora', temporary: False, role: 'estonian_text_corpora_read'\n",
      "andmebaasi salvestatud kollokatsioonid kollektsioonidest: 0 - 100000\n",
      "andmebaasi salvestatud kollokatsioonid kollektsioonidest: 0 - 159744\n",
      "20230415-125312 Done.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import sys\n",
    "from textwrap import wrap\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "# estnltk_patches teegi asukoht\n",
    "# https://github.com/estnltk/syntax_experiments/tree/adverbials/adverbials/estnltk_patches\n",
    "# sys.path.append('/Users/rabauti/repos/syntax_experiments_adverbial/syntax_experiments/adverbials')\n",
    "# for hpc (copy estnltk_patches to that location)\n",
    "sys.path.append('/gpfs/space/home/zummy/v10_verb_compound_obl')\n",
    "from estnltk_patches import EntityTagger\n",
    "from estnltk.storage.postgres import PostgresStorage\n",
    "\n",
    "# serialisation_registry fail \n",
    "# https://github.com/estnltk/syntax_experiments/blob/syntax_consistency/collection_splitting/serialisation_module/syntax_v1.py\n",
    "from estnltk_core.converters.serialisation_registry import SERIALISATION_REGISTRY\n",
    "from estnltk_patches import syntax_v1 \n",
    "\n",
    "if 'syntax_v1' not in SERIALISATION_REGISTRY:\n",
    "    SERIALISATION_REGISTRY['syntax_v1'] = syntax_v1\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stdout, **kwargs)\n",
    "\n",
    "\n",
    "# Abimeetodid tööks graafidega\n",
    "class GraphMethods:\n",
    "    # kahe listi ühisosa\n",
    "    @staticmethod\n",
    "    def intersection(a, b):\n",
    "        return list(set(a).intersection(b))\n",
    "\n",
    "    # tipu leidmine atribuudi väärtuse järgi\n",
    "    @staticmethod\n",
    "    def get_nodes_by_attributes(graph, attrname, attrvalue):\n",
    "        nodes = defaultdict(list)\n",
    "        {nodes[v].append(k) for k, v in nx.get_node_attributes(graph, attrname).items()}\n",
    "        if attrvalue in nodes:\n",
    "            return dict(nodes)[attrvalue]\n",
    "        return []\n",
    "\n",
    "    # graafi joonistamine\n",
    "    # tipp - lemma\n",
    "    # serv - deprel\n",
    "    @staticmethod\n",
    "    def draw_graph(graph, **kwargs):\n",
    "        \"\"\"Puu/graafi joonistamine\n",
    "        tipp - lemma\n",
    "        serv - deprel\n",
    "        title string    Graafi pealkiri\n",
    "        filename string Failinimi kuhu joonis salvestatakse\n",
    "        highlight array of integers     tippude id, d mis värvitakse joonisel punaseks\n",
    "        \"\"\"\n",
    "        title = None\n",
    "        filename = None\n",
    "        custom_colors = None\n",
    "        highlight = []\n",
    "        if 'title' in kwargs:\n",
    "            title = kwargs['title']\n",
    "\n",
    "        if 'filename' in kwargs:\n",
    "            filename = kwargs['filename']\n",
    "\n",
    "        if 'highlight' in kwargs:\n",
    "            highlight = kwargs['highlight']\n",
    "\n",
    "        if 'custom_colors' in kwargs:\n",
    "            custom_colors = kwargs['custom_colors']\n",
    "\n",
    "        if not custom_colors:\n",
    "            colors = ['lightskyblue' for node in graph]\n",
    "        else:\n",
    "            colors = custom_colors\n",
    "        # soovitud tipud punaseks\n",
    "\n",
    "        color_map = ['red' if node in highlight else colors[i] for (i,node) in enumerate(graph.nodes)]\n",
    "\n",
    "        #print (color_map)\n",
    "        # joonise suurus, et enamik puudest ära mahuks\n",
    "        plt.rcParams[\"figure.figsize\"] = (18.5, 10.5)\n",
    "\n",
    "        #pealkiri\n",
    "        if title:\n",
    "            title = (\"\\n\".join(wrap( title, 120)))\n",
    "            plt.title(title)\n",
    "\n",
    "        pos = graphviz_layout(graph, prog='dot')\n",
    "        labels = nx.get_node_attributes(graph, 'lemma')\n",
    "        nx.draw(graph, pos, cmap = plt.get_cmap('jet'),labels=labels, with_labels=True, node_color=color_map)\n",
    "        edge_labels = nx.get_edge_attributes(graph, 'deprel')\n",
    "        nx.draw_networkx_edge_labels(graph, pos, edge_labels)\n",
    "\n",
    "        #kui failinimi, siis salvestame faili\n",
    "        #kui pole, siis joonistame väljundisse\n",
    "        if filename:\n",
    "            plt.savefig(f'{filename}.png', dpi=100)\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "    @staticmethod\n",
    "    def stanza_syntax_to_graph(sentence_syntax_layer):\n",
    "        \"\"\" stanza stanza_syntax objektist graafi tegemine \"\"\"\n",
    "        g_sentence = nx.DiGraph()\n",
    "        for data in sentence_syntax_layer:\n",
    "            if isinstance(data['id'], int):\n",
    "                # paneme graafi kokku\n",
    "                g_sentence.add_node(data['id'], id=data['id'], lemma=data['lemma'], pos=data['upostag'],\n",
    "                                    deprel=data['deprel'],\n",
    "                                    form=data.text, feats=data['feats'], start=data.start, end=data.end)\n",
    "\n",
    "                g_sentence.add_edge(data['id'] - data['id'] + data['head'], data['id'], deprel=data['deprel'])\n",
    "        return g_sentence\n",
    "\n",
    "\n",
    "class DbMethods:\n",
    "\n",
    "    @staticmethod\n",
    "    def prep_coll_db(do_truncate = True):\n",
    "        \"\"\" loob vajalikud db tabelid ja indeksid \"\"\"\n",
    "        global TABLENAME, cursor, conn\n",
    "\n",
    "        cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS collections_processed\n",
    "                        (tablename text, lastcollection integer);\n",
    "                        \"\"\")\n",
    "\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE UNIQUE INDEX IF NOT EXISTS collections_processed_uniq ON collections_processed(tablename);\n",
    "      \"\"\")\n",
    "\n",
    "        # tsv failist lugemise korral loome tabeli alati nullist\n",
    "        cursor.execute(f\"\"\"\n",
    "          INSERT INTO collections_processed VALUES (?,?)\n",
    "          ON CONFLICT(tablename) DO UPDATE SET lastcollection=?;\"\"\", (TABLENAME, 0, 0,))\n",
    "\n",
    "        cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {TABLENAME}\n",
    "                        (`id` INTEGER PRIMARY KEY AUTOINCREMENT\n",
    "                        , `verb` text\n",
    "                        , `verb_compound` text\n",
    "                        , `obl_root` text\n",
    "                        , `obl_case` text\n",
    "                        , `ner_loc` text\n",
    "                        , `ner_per` text\n",
    "                        , `ner_org` text\n",
    "                        , `timex` text\n",
    "                        \n",
    "                        , `phrase_type`\n",
    "                        , `count` int\n",
    "                        );\n",
    "                       \"\"\")\n",
    "        \n",
    "        # add uniq_index on all fields beside id and total\n",
    "        INDEXNAME = f'{TABLENAME}_unique'\n",
    "        cursor.execute(f\"\"\"CREATE UNIQUE INDEX IF NOT EXISTS {INDEXNAME}\n",
    "          ON {TABLENAME}(\n",
    "                `verb`\n",
    "                , `verb_compound` \n",
    "                , `obl_root` \n",
    "                , `obl_case` \n",
    "                , `ner_loc` \n",
    "                , `ner_per` \n",
    "                , `ner_org` \n",
    "                , `timex` \n",
    "              );\n",
    "          \"\"\")\n",
    "        # loome näidete faili\n",
    "        cursor.execute(f\"\"\"CREATE TABLE {TABLENAME}_examples\n",
    "                        (colloc_id integer\n",
    "                        , sentences text);\n",
    "                        \"\"\")\n",
    "\n",
    "        # tsv failist lugemise korral loome tabeli alati nullist\n",
    "        cursor.execute(f\"\"\"DELETE FROM {TABLENAME};\"\"\")\n",
    "\n",
    "       \n",
    "        \n",
    "        INDEXNAME = f'{TABLENAME}_unique'\n",
    "        cursor.execute(f\"\"\"CREATE UNIQUE INDEX IF NOT EXISTS {INDEXNAME}_examples\n",
    "          ON {TABLENAME}_examples(\n",
    "              colloc_id\n",
    "              );\n",
    "          \"\"\")\n",
    "        \n",
    "        if do_truncate: cursor.execute(f\"DELETE FROM {TABLENAME} WHERE 1;\")\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "    @staticmethod\n",
    "    def save_coll_to_db(collocations, lastcollection):\n",
    "        \"\"\" salvestab andmed db tabelisse \"\"\"\n",
    "        \n",
    "        global TABLENAME, cursor, conn, cases\n",
    "        sql_colls = []\n",
    "        sql_examples = []\n",
    "        for key in collocations.keys():\n",
    "            #total = some of cases + opposite case \n",
    "            \n",
    "            sql_colls.append((key[0], # verb\n",
    "                              key[1], # verb_compound\n",
    "                              key[2], # obl_root\n",
    "                              key[3], # obl_case\n",
    "                              key[4], # ner_loc\n",
    "                              key[5], # ner_per\n",
    "                              key[6], # ner_org\n",
    "                              key[7], # timex\n",
    "                              collocations[key]['total'] # count\n",
    "                              ))\n",
    "            \n",
    "            if len(collocations[key]['examples']):\n",
    "                sql_examples.append((key[0], # verb\n",
    "                                      key[1], # verb_compound\n",
    "                                      key[2], # obl_root\n",
    "                                      key[3], # obl_case\n",
    "                                      key[4], # ner_loc\n",
    "                                      key[5], # ner_per\n",
    "                                      key[6], # ner_org\n",
    "                                      key[7], # timex\n",
    "                                        ','.join([str(example) for example in collocations[key]['examples']])\n",
    "                                        ))\n",
    "                \n",
    "        cursor.executemany(f\"\"\"\n",
    "        INSERT INTO {TABLENAME} (\n",
    "            verb\n",
    "            , verb_compound\n",
    "            , obl_root\n",
    "            , obl_case\n",
    "            , ner_loc\n",
    "            , ner_per\n",
    "            , ner_org\n",
    "            , timex\n",
    "            , count )\n",
    "            \n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(`verb`\n",
    "                , `verb_compound` \n",
    "                , `obl_root` \n",
    "                , `obl_case` \n",
    "                , `ner_loc` \n",
    "                , `ner_per` \n",
    "                , `ner_org` \n",
    "                , `timex` )\n",
    "            DO UPDATE SET\n",
    "                `count` = `count` + excluded.`count`\n",
    "                ;\n",
    "        \"\"\", sql_colls)\n",
    "        \n",
    "        cursor.executemany(f\"\"\"\n",
    "        INSERT INTO {TABLENAME}_examples (colloc_id, sentences)\n",
    "            VALUES ( \n",
    "                (SELECT id FROM {TABLENAME} WHERE \n",
    "                    verb = ? AND verb_compound = ? AND obl_root = ? AND obl_case = ? AND ner_loc = ? AND ner_per = ? AND ner_org = ? AND timex = ? ),\n",
    "                ?)\n",
    "        ON CONFLICT(colloc_id)\n",
    "            DO UPDATE SET\n",
    "                sentences = sentences || ',' || excluded.sentences\n",
    "                ;\n",
    "        \"\"\", sql_examples)\n",
    "\n",
    "        cursor.execute(f\"\"\"\n",
    "          INSERT INTO collections_processed VALUES (?,?)\n",
    "          ON CONFLICT(tablename) DO UPDATE SET lastcollection=?;\"\"\", (TABLENAME, lastcollection, lastcollection,))\n",
    "\n",
    "        conn.commit()\n",
    "        eprint(f'andmebaasi salvestatud kollokatsioonid kollektsioonidest: 0 - {lastcollection}')\n",
    "\n",
    "\n",
    "# helping functions\n",
    "def feats_get_case(arr):\n",
    "    \"\"\"\n",
    "    https://github.com/EstSyntax/EstCG/ (käänded)\n",
    "    \"\"\"\n",
    "    for attr in arr:\n",
    "        if attr in ('nom',  # nimetav\n",
    "                    'gen',  # omastav\n",
    "                    'part',  # osastav\n",
    "                    'adit',  # lyh sisse\n",
    "                    'ill',  # sisse\n",
    "                    'in',  # sees\n",
    "                    'el',  # seest\n",
    "                    'all',  # alale\n",
    "                    'ad',  # alal\n",
    "                    'abl',  # alalt\n",
    "                    'tr',  # saav\n",
    "                    'term',  # rajav\n",
    "                    'es',  # olev\n",
    "                    'abes',  # ilma#\n",
    "                    'kom',  # kaasa#\n",
    "                    ):\n",
    "            return attr\n",
    "    return '<käändumatu>'\n",
    "\n",
    "\n",
    "def do_skip_verb(graph, verb):\n",
    "    \"\"\" \n",
    "    funktsioon verbide filtreerimiseks\n",
    "    jäetakse vahele need verbid, millel pole \"kindlat\" aega ja isikut\n",
    "    \"\"\"\n",
    "    feats = graph.nodes[verb]['feats'].keys()\n",
    "    \n",
    "    # kui on umbisikuline\n",
    "    if 'imps' in feats:\n",
    "        #GraphMethods.draw_graph(graph, title=' '.join([graph.nodes[n]['form'] for n in sorted(graph.nodes) if n ]), highlight=[verb])\n",
    "        return True\n",
    "    \n",
    "    #tense pole past, impf, pres\n",
    "    if not len(GraphMethods.intersection(['past', 'impf', 'pres'], feats)):\n",
    "        #print(graph.nodes[verb])\n",
    "        return True\n",
    "       \n",
    "    return False\n",
    "\n",
    "# seda tabeli väljade massiivi kasutatakse hiljem tabelis indeksite loomiseks\n",
    "key_fields = ('verb'\n",
    "                , 'verb_compound'\n",
    "                , 'obl_root'\n",
    "                , 'obl_case'\n",
    "                , 'ner_loc'\n",
    "                , 'ner_per'\n",
    "                , 'ner_org'\n",
    "                , 'timex' ,\n",
    "                )\n",
    "\n",
    "# suhte prioriteet\n",
    "relations = ['match', 'contains', 'inside', 'intersects', '-']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "-:            OBL ei ole kautud ühegi spaniga\n",
    "match:       OBL span langeb kokku NER/TIMEX spaniga\n",
    "contains:    OBL spani sees on NER/TIMEX span\n",
    "inside:        OBL span on  NER/TIMEX spani sees\n",
    "intersects:  OBL span lõikub NER/TIMEX spaniga\n",
    "\n",
    "\"\"\"\n",
    "def get_relation_type(obl_nodes, other_nodes):\n",
    "    \"\"\"\n",
    "    Tagastab obl fraasi suhte teise fraasi suhtes\n",
    "    \"\"\"\n",
    "    obl_nodes = sorted(obl_nodes)\n",
    "    other_nodes = sorted(other_nodes)\n",
    "    \n",
    "    # kui obl_nodes tühi, ei tohiks tegelikult olla sellist olukorda\n",
    "    if not len(obl_nodes) or not len(other_nodes):\n",
    "        return '-'\n",
    "    \n",
    "    # match: OBL span langeb kokku NER/TIMEX spaniga\n",
    "    if len(obl_nodes) and obl_nodes == other_nodes:\n",
    "        return 'match'\n",
    "    \n",
    "    # ühisosa\n",
    "    intersection = sorted(GraphMethods.intersection(obl_nodes, other_nodes))\n",
    "    \n",
    "    # contains:    OBL spani sees on NER/TIMEX span\n",
    "    if intersection == other_nodes and len(obl_nodes)>len(other_nodes):\n",
    "        return 'contains'\n",
    "    \n",
    "    # inside:        OBL span on  NER/TIMEX spani sees\n",
    "    if intersection == obl_nodes and len(other_nodes)>len(obl_nodes):\n",
    "        return 'inside'\n",
    "    \n",
    "    # intersects:  OBL span lõikub NER/TIMEX spaniga\n",
    "    if len(intersection):\n",
    "        return 'intersects'\n",
    "    \n",
    "    # -: OBL ei ole kautud ühegi spaniga\n",
    "    return '-'\n",
    "    \n",
    "\n",
    "examples_combinations = []\n",
    "def extract_something(text, colId, collocations):\n",
    "    \"\"\"\n",
    "       kogub lausest kokku kollokatsioonid ja näitelausete collectionId-d\n",
    "       text - esntltk Text (1 lause) + kihid \n",
    "           'v171_named_entities', \n",
    "           'v172_stanza_syntax', \n",
    "           'v172_obl_phrases', \n",
    "           'v172_pre_timexes'\n",
    "       colId - lause collectionId andmebaasis\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    keys = []\n",
    "    # sentence should contain layers: 'v171_named_entities', 'v172_stanza_syntax', 'v172_obl_phrases', 'v172_pre_timexes'\n",
    "    # * v171_named_entities  - NER (types:  )\n",
    "    # * v172_obl_phrases - OBL\n",
    "    # * v172_pre_timexes - TIMEX\n",
    "    # * v172_stanza_syntax  - Stanza märgendus\n",
    "    \n",
    "    # ---\n",
    "    # 1. make directed networx graph \n",
    "    graph = GraphMethods.stanza_syntax_to_graph(text.v172_stanza_syntax)\n",
    "    \n",
    "    # shortest path between nodes\n",
    "    path = nx.all_pairs_shortest_path_length(graph)\n",
    "    \n",
    "    # matrix for node distances \n",
    "    dpath = {x[0]: x[1] for x in path}\n",
    "    \n",
    "    # ---\n",
    "    # 2. collect verbs, compounds node ids\n",
    "    \n",
    "    # verb nodes\n",
    "    verb_nodes = GraphMethods.get_nodes_by_attributes(graph, attrname='pos', attrvalue='V')\n",
    "    \n",
    "    # compound:prt\n",
    "    compound_nodes = GraphMethods.get_nodes_by_attributes(graph, attrname='deprel', attrvalue='compound:prt')\n",
    "    \n",
    "    # ---\n",
    "    # 3. collect OBL info\n",
    "    obl_data = []\n",
    "    for obl in text['v172_obl_phrases']:\n",
    "        obl_data.append ({\n",
    "            'nodes': [GraphMethods.get_nodes_by_attributes(graph, attrname='start', attrvalue=s.start)[0] for s in obl.spans],\n",
    "            'root_id': obl.root_id,\n",
    "            'root_lemma': graph.nodes[obl.root_id]['lemma'],\n",
    "            'root_case': feats_get_case(graph.nodes[obl.root_id]['feats'])\n",
    "        })\n",
    "        \n",
    "    # ---\n",
    "    # 4. collect NER info\n",
    "    ner_data = []\n",
    "    for ner in text['v171_named_entities']:\n",
    "        start_nodes = [GraphMethods.get_nodes_by_attributes(graph, attrname='start', attrvalue=s.start)[0] for s in ner.spans]\n",
    "        end_nodes = [GraphMethods.get_nodes_by_attributes(graph, attrname='end', attrvalue=s.end)[0] for s in ner.spans]\n",
    "        if not start_nodes == end_nodes:\n",
    "            display (text.words)\n",
    "            print (ner,  f'ner.start: {ner.start}', f'ner.end: {ner.end}' )\n",
    "            raise ('NER not start_nodes == end_nodes')\n",
    "            \n",
    "        ner_data.append( {\n",
    "            'tag': ner.nertag, \n",
    "            'nodes': start_nodes\n",
    "            \n",
    "        })\n",
    "        \n",
    "    #----\n",
    "    # 5. collect TIMEX info\n",
    "    timex_data = []\n",
    "    for timex in text['v172_pre_timexes']:\n",
    "        \n",
    "        # timex span can begin and end in the middle of words\n",
    "        # span.end and span.begin in some cases do not match end and start of word spans\n",
    "        # first we try to find exact match and if it doesn't work we find nearest matched end and start of word spans\n",
    "        # \n",
    "        try:\n",
    "            first_node = GraphMethods.get_nodes_by_attributes(graph, attrname='start', attrvalue=timex.start)[0]\n",
    "        except:\n",
    "            # last node that starts before timex span starts\n",
    "            first_node = [n for n in graph.nodes if n and graph.nodes[n]['start']<timex.start][-1]\n",
    "\n",
    "        try:\n",
    "            last_node = GraphMethods.get_nodes_by_attributes(graph, attrname='end', attrvalue=timex.end)[0]\n",
    "        except:\n",
    "            # fist node that ends after timex span ends\n",
    "            last_node = [n for n in graph.nodes if n and graph.nodes[n]['end']>timex.start][0]\n",
    "            \n",
    "\n",
    "        timex_data.append({\n",
    "            'type': timex.type, \n",
    "            'nodes': list(range(first_node, last_node+1))\n",
    "        })\n",
    "        \n",
    "\n",
    "    # iteratsioon üle verbide\n",
    "    # verbi compound alluvad kogutakse kokku ja järjestatakse alfabeetiliselt\n",
    "    # itereeritakse üle obl fraaside, kus fraasi juur on selle verbi alluv (ükskõik, kui kaugel verbist)  \n",
    "    # timex ja ner kohta pannakse tabelisse kõige prioriteetsem seos \n",
    "    # seoste prioriteet on paigas massiivis relations\n",
    "    \n",
    "    #key = (verb_lemma, verb_compound, obl_lemma, obl_case, ner_loc, ner_per, ner_org, timex)\n",
    "    for verb in verb_nodes:\n",
    "        \n",
    "        # do skip collocation if verb is \"unusual\"\n",
    "        if do_skip_verb(graph, verb): continue\n",
    "        \n",
    "        # childnodes\n",
    "        kids = [k for k in dpath[verb] if dpath[verb][k] == 1]\n",
    "        v_lemma = graph.nodes[verb]['lemma']\n",
    "        \n",
    "        # compound children\n",
    "        n_compounds = GraphMethods.intersection(kids, compound_nodes)\n",
    "        if not len(n_compounds):\n",
    "            verb_compound = ''\n",
    "            n_compounds.append(None)\n",
    "        else: \n",
    "            verb_compound = ', '.join([graph.nodes[n]['lemma'] for n in sorted(n_compounds) if n ])\n",
    "        \n",
    "        # if obl_data is empty, there is no need to further check\n",
    "        if not len(obl_data):\n",
    "            #key = (verb_lemma, verb_compound, obl_lemma, obl_case, ner_loc, ner_per, ner_org, timex)\n",
    "            keys.append( (v_lemma , verb_compound, '' , '', '', '', '', '', ))\n",
    "            continue\n",
    "        \n",
    "        for obl in obl_data:\n",
    "            if not obl['root_id'] in kids: continue\n",
    "            ner_relations = {'LOC':[], 'PER':[], 'ORG':[]}\n",
    "            for ner in ner_data:\n",
    "                if ner['tag'] not in ner_relations:\n",
    "                    ner_relations[ner['tag']] = []\n",
    "                ner_relations[ner['tag']].append(get_relation_type( obl['nodes'], ner['nodes']))\n",
    "            \n",
    "            timex_relations = []\n",
    "            for timex in timex_data:\n",
    "                timex_relations.append(get_relation_type( obl['nodes'], timex['nodes']))\n",
    "            \n",
    "            key = (v_lemma, \n",
    "                   verb_compound, \n",
    "                   obl['root_lemma'], \n",
    "                   obl['root_case'], \n",
    "                   ner_relations['LOC'][0] if len(ner_relations['LOC']) else '-' , \n",
    "                   ner_relations['PER'][0] if len(ner_relations['PER']) else '-', \n",
    "                   ner_relations['ORG'][0] if len(ner_relations['ORG']) else '-', \n",
    "                   timex_relations[0] if len(timex_relations) else '-' , ) \n",
    "            keys.append( key)\n",
    "\n",
    "    for key in keys:\n",
    "        if not key in collocations:\n",
    "            collocations[key] = {'total': 0, 'examples': []}\n",
    "        collocations[key]['total'] += 1\n",
    "        collocations[key]['examples'].append(colId)\n",
    "   \n",
    "    return collocations,\n",
    "    \n",
    "\n",
    "\n",
    "date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "eprint(f'{date_time} Start.')\n",
    "collectionName = 'koondkorpus_sentences' \n",
    "#collectionName = 'koondkorpus_sentences_test_5000_sg_thread'  # \n",
    "\n",
    "\n",
    "TYPE = 'verb_compound_obl'\n",
    "TABLENAME = f'{TYPE}_{collectionName}'\n",
    "BATCH_SIZE = 100000\n",
    "\n",
    "date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "conn = sqlite3.connect(f\"v10_{collectionName}_{TYPE}_collocations_{date_time}.db\")  #\n",
    "\n",
    "cursor = conn.cursor()\n",
    "DbMethods.prep_coll_db()\n",
    "\n",
    "storage = PostgresStorage(pgpass_file='~/.pgpass',\n",
    "                          schema='estonian_text_corpora',\n",
    "                          role='estonian_text_corpora_read',\n",
    "                          temporary=False)\n",
    "\n",
    "collection = storage[collectionName]\n",
    "\n",
    "\n",
    "collocations = {}\n",
    "\n",
    "\n",
    "collection.selected_layers = ['v171_named_entities', 'v172_stanza_syntax', 'v172_obl_phrases', 'v172_pre_timexes']\n",
    "for (colId, text) in collection.select(progressbar=None, layers=['v171_named_entities', 'v172_stanza_syntax', 'v172_obl_phrases', 'v172_pre_timexes'], return_index=True):\n",
    "\n",
    "    # viimane lause\n",
    "    collocations,  = extract_something(text, colId, collocations )\n",
    "    if not colId == 0 and not colId % BATCH_SIZE:\n",
    "        DbMethods.save_coll_to_db(collocations, colId)\n",
    "        collocations = {}\n",
    "\n",
    "# saving last batch\n",
    "DbMethods.save_coll_to_db(collocations , colId)\n",
    "\n",
    "indexesQ = []\n",
    "\n",
    "for field in list(key_fields) + ['count', 'phrase_type']:\n",
    "    direction = 'ASC' if field not in ['count'] else 'DESC'\n",
    "\n",
    "        \n",
    "    indexesQ.append (f'CREATE INDEX IF NOT EXISTS \"`{field}`\" ON \"{TABLENAME}\" (\"`{field}`\" {direction});')\n",
    "    \n",
    "for q in indexesQ:\n",
    "    cursor.execute(q)\n",
    "\n",
    "cursor.execute(f\"SELECT count(*) FROM {TABLENAME}\")\n",
    "all_collocations = cursor.fetchall()\n",
    "\n",
    "date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "eprint(f'{date_time} Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b13ae-9ea9-4f08-96ad-b1bb2e97ec0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estnltk_collocations_py38",
   "language": "python",
   "name": "estnltk_collocations_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
