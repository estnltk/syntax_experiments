{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce5558f-700f-4379-9883-7ebd83f15728",
   "metadata": {},
   "source": [
    "# Kohakäänetega lausetekstide andmebaasist lugemise test\n",
    "\n",
    "See notebook on testimiseks. Skript, millega suuremas mahus anmeid baasist loetakse on <code>v21_fetch_examples.py</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96230578-84b7-40e6-a54e-6c3df40b7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from data_helpers.syntax_graph import SyntaxGraph\n",
    "from data_helpers.db_reader import DbReader\n",
    "\n",
    "\n",
    "def get_span(graph, nodes, label):\n",
    "    spans = get_spans(graph, nodes, label)\n",
    "    if len(spans) == 1:\n",
    "        return spans[0]\n",
    "    return spans\n",
    "\n",
    "\n",
    "def get_spans(graph, nodes, label):\n",
    "    spans = []\n",
    "    for n in nodes:\n",
    "        spans.append({\n",
    "            'start': graph.nodes[n]['start'],\n",
    "            'end': graph.nodes[n]['end'],\n",
    "            'text': graph.nodes[n]['form'],\n",
    "            'labels': [label]})\n",
    "    \n",
    "    return spans\n",
    "\n",
    "def get_first(string, sep=','):\n",
    "    if not string: return string\n",
    "    return string.split(sep)[0]\n",
    "\n",
    "def get_last(string, sep=','):\n",
    "    if not string: return string\n",
    "    return string.split(sep)[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4ab6d-4090-4fd0-8120-fca1626254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init db\n",
    "# takes ca 2 mins\n",
    "#collection_name = 'koondkorpus_sentences'\n",
    "collection_name = 'koondkorpus_sentences_test_5000_sg_thread'\n",
    "\n",
    "my_db_reader = DbReader(\n",
    "    pgpass_file='~/.pgpass',\n",
    "    schema='estonian_text_corpora',\n",
    "    role='estonian_text_corpora_read',\n",
    "    temporary=False,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "!mkdir result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d70a74-77b1-41c0-bffe-e796fa520bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect files\n",
    "datestamp = '20230824-103951'\n",
    "directory = 'result/'\n",
    "files = []\n",
    "# get all files inside a specific folder\n",
    "for path in os.scandir(directory):\n",
    "    if path.is_file() and re.match(r'^' + datestamp + '_sentence_ids_for_[a-z_]+.csv$', path.name):\n",
    "        print(path.name)\n",
    "        files.append(path.name)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82688c66-8282-435a-98f9-cdf4b393f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_delimiter = ','\n",
    "csv_quotechar='\"'\n",
    "csv_quoting=csv.QUOTE_NONNUMERIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1dff68-76a3-44cf-b214-f1aa295cbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite logic, so result will be written to csv file directly, not with pandas\n",
    "\n",
    "for f in files:\n",
    "    print('------------------')\n",
    "    print(datetime.now().strftime(\"%Y%m%d-%H%M%S\"), 'Start', directory+f)\n",
    "    \n",
    "   \n",
    "    df_final = pd.read_csv(directory+f, dtype = {'compound_ids': str})\n",
    "    \n",
    "    \n",
    "    df_final['verb_span'] = ''\n",
    "    \n",
    "    df_final['obl_case1'] = df_final['cases_list'].apply(get_first)\n",
    "    df_final['obl_case2'] = df_final['cases_list'].apply(get_last)\n",
    "    df_final['obl_root1'] = df_final['oblroot_list'].apply(get_first)\n",
    "    df_final['obl_root2'] = df_final['oblroot_list'].apply(get_last)\n",
    "    df_final['obl_ids1'] = df_final['oblids_list'].apply(get_first, ':')\n",
    "    df_final['obl_ids2'] = df_final['oblids_list'].apply(get_last, ':')\n",
    "    \n",
    "    df_final['obl_span1'] = ''\n",
    "    df_final['obl_span2'] = ''\n",
    "    df_final['obl_lemma2'] = ''\n",
    "    df_final['obl_lemma1'] = ''\n",
    "    df_final['sentence'] = ''\n",
    "    df_final['oblp_spans1'] = ''\n",
    "    df_final['oblp_spans2'] = ''\n",
    "    df_final = df_final.fillna('')\n",
    "    \n",
    "    my_dict = df_final.to_dict(orient='records')\n",
    "    \n",
    "    \n",
    "    date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    outputfile = f.replace('sentence_ids', 'sentences')\n",
    "    \n",
    "    # Fetching from database\n",
    "    sentence_ids = [int(sent_id) for sent_id in list(df_final['sentence_id'].unique())]\n",
    "    uniq_sentences_total = len(sentence_ids)\n",
    "    print(f'Total sentences to fetch: {uniq_sentences_total}')\n",
    "    \n",
    "    out_file = open(directory+outputfile, 'w')\n",
    "    csvwriter = csv.writer(out_file, delimiter=csv_delimiter, quotechar=csv_quotechar, quoting=csv_quoting)\n",
    "\n",
    "    csvwriter.writerow([ c for c in my_dict[0].keys()])\n",
    "    \n",
    "    my_db_reader.set_layers(['v172_stanza_syntax'])\n",
    "\n",
    "    BATCH = 500 # batch for reading sentences\n",
    "\n",
    "    first = 0\n",
    "    all_sentence_ids = df_final['sentence_id'].tolist()\n",
    "    all_sentence_ids = [int(sent_id) for sent_id in all_sentence_ids]\n",
    "\n",
    "    \n",
    "    \n",
    "    for batch_nr in range(round(df_final.shape[0]/BATCH)+1):\n",
    "        \n",
    "        \n",
    "        date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        print(f'{date_time} fetching batch {batch_nr}')\n",
    "       \n",
    "        \n",
    "        first = batch_nr * BATCH\n",
    "        last = first + BATCH\n",
    "        \n",
    "        if last > df_final.shape[0]:\n",
    "            last = df_final.shape[0]\n",
    "\n",
    "        # print(first, last, batch_nr)\n",
    "        batch_sentence_ids = all_sentence_ids[first:last]\n",
    "\n",
    "        batch_sentences = {}\n",
    "        print(f'start fetching', len(batch_sentence_ids))\n",
    "        for collection_id, text in my_db_reader.get_collections(shuffle=False, progressbar='ascii', col_ids=batch_sentence_ids):\n",
    "            batch_sentences[collection_id] = text\n",
    "\n",
    "        date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        print(f'{date_time} batch fetched {batch_nr}')\n",
    "        \n",
    "        \n",
    "        for row_nr in range(first, last):\n",
    "            \n",
    "\n",
    "            sentence_id = df_final['sentence_id'][row_nr]\n",
    "            verb_id = int(df_final['verb_id'][row_nr])\n",
    "            obl_root1 = int(df_final['obl_root1'][row_nr])\n",
    "            obl_root2 = int(df_final['obl_root2'][row_nr])\n",
    "            compound_ids = [int(n) for n in df_final['compound_ids'][row_nr].split(',') if n.isdigit()]\n",
    "            obl_ids1 = [int(n) for n in df_final['obl_ids1'][row_nr].split(',') if n.isdigit()]\n",
    "            obl_ids2 = [int(n) for n in df_final['obl_ids2'][row_nr].split(',') if n.isdigit()]\n",
    "            sentence_text = batch_sentences[sentence_id].text\n",
    "\n",
    "            g = SyntaxGraph(batch_sentences[sentence_id]['v172_stanza_syntax'])\n",
    "\n",
    "            my_row = my_dict[row_nr]\n",
    "            \n",
    "            \n",
    "            my_row['obl_lemma1'] = g.nodes[obl_root1]['lemma']\n",
    "            my_row['obl_lemma2'] = g.nodes[obl_root2]['lemma']\n",
    "\n",
    "            \n",
    "\n",
    "            my_row['sentence'] = str(batch_sentences[sentence_id].text)\n",
    "\n",
    "\n",
    "            my_row['verb_span'] = json.dumps(get_span(g, [verb_id], 'V'), ensure_ascii=False)\n",
    "            my_row['obl_span1'] = json.dumps(get_span(g, [obl_root1], 'OBL'), ensure_ascii=False)\n",
    "            my_row['obl_span2'] = json.dumps(get_span(g, [obl_root2], 'OBL'), ensure_ascii=False)\n",
    "            \n",
    "            my_row['compound_spans'] = json.dumps(get_spans(g, compound_ids, 'COMPOUND'), ensure_ascii=False)\n",
    "            my_row['oblp_spans1'] = json.dumps(get_spans(g, obl_ids1, 'OBLP'), ensure_ascii=False)\n",
    "            my_row['oblp_spans2'] = json.dumps(get_spans(g, obl_ids2, 'OBLP'), ensure_ascii=False)\n",
    "            \n",
    "            my_row['obl_lemma1'] = g.nodes[obl_root1]['lemma']\n",
    "            my_row['obl_lemma2'] = g.nodes[obl_root2]['lemma']\n",
    "            \n",
    "            row = [ my_row[c] for c in my_row.keys()]\n",
    "            csvwriter.writerow(row)            \n",
    "        \n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd2bf1-38ed-4145-914b-58a2a487e344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estnltk_collocations_py38",
   "language": "python",
   "name": "estnltk_collocations_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
